{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5s843OCCmnv"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL4a1qfzCopu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import json\n",
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import threading\n",
        "import pytransform3d.camera as pc\n",
        "import pytransform3d.transformations as pt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Download the images\n",
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "REPO_URL = \"https://github.com/Tiromachelan/camera-pose-estimator\"\n",
        "\n",
        "if in_colab():\n",
        "  if not Path(\"camera-pose-estimator/images\").exists():\n",
        "    !git clone {REPO_URL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoTpoxngChX0"
      },
      "source": [
        "## Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7KtYIwDBXfQ",
        "outputId": "b6418ace-0217-4e2d-b82e-4fda0a2f69f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calibrating data in folder images\n",
            "Processed images/DSCN0504.jpeg\n",
            "Processed images/DSCN0494.jpeg\n",
            "Processed images/DSCN0498.jpeg\n",
            "Processed images/DSCN0499.jpeg\n",
            "Processed images/DSCN0494.jpeg\n",
            "Processed images/DSCN0498.jpeg\n",
            "Processed images/DSCN0499.jpeg\n",
            "Processed images/DSCN0495.jpeg\n",
            "Processed images/DSCN0483.jpeg\n",
            "Processed images/DSCN0488.jpeg\n",
            "Processed images/DSCN0495.jpeg\n",
            "Processed images/DSCN0483.jpeg\n",
            "Processed images/DSCN0488.jpeg\n",
            "Processed images/DSCN0502.jpeg\n",
            "Processed images/DSCN0484.jpeg\n",
            "Processed images/DSCN0492.jpeg\n",
            "Processed images/DSCN0502.jpeg\n",
            "Processed images/DSCN0484.jpeg\n",
            "Processed images/DSCN0492.jpeg\n",
            "Processed images/DSCN0493.jpeg\n",
            "Processed images/DSCN0485.jpeg\n",
            "Processed images/DSCN0503.jpeg\n",
            "Processed images/DSCN0493.jpeg\n",
            "Processed images/DSCN0485.jpeg\n",
            "Processed images/DSCN0503.jpeg\n",
            "Processed images/DSCN0489.jpeg\n",
            "Processed images/DSCN0489.jpeg\n",
            "Processed images/DSCN0490.jpeg\n",
            "Processed images/DSCN0486.jpeg\n",
            "Processed images/DSCN0490.jpeg\n",
            "Processed images/DSCN0486.jpeg\n",
            "Processed images/DSCN0501.jpeg\n",
            "Processed images/DSCN0487.jpeg\n",
            "Processed images/DSCN0501.jpeg\n",
            "Processed images/DSCN0487.jpeg\n",
            "Processed images/DSCN0496.jpeg\n",
            "Processed images/DSCN0497.jpeg\n",
            "Processed images/DSCN0496.jpeg\n",
            "Processed images/DSCN0497.jpeg\n"
          ]
        }
      ],
      "source": [
        "class Calibration:\n",
        "    @staticmethod\n",
        "    def calibrateCamera(image_files):\n",
        "        print(f\"calibrating data in folder {image_files}\")\n",
        "        # termination criteria\n",
        "        criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "\n",
        "        # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
        "        pattern_size = (9, 6)\n",
        "        objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)\n",
        "        objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)\n",
        "        # Arrays to store object points and image points from all the images.\n",
        "        objpoints = [] # 3d point in real world space\n",
        "        imgpoints = [] # 2d points in image plane.\n",
        "\n",
        "        images = glob.glob(image_files + '/*.jpeg')\n",
        "\n",
        "        gray = None\n",
        "        for fname in images:\n",
        "            img = cv.imread(fname)\n",
        "            gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "            #print(f\"{'processing image '}{fname}\")\n",
        "\n",
        "            # Find the chess board corners\n",
        "            ret, corners = cv.findChessboardCorners(gray, pattern_size, None)\n",
        "\n",
        "            # If found, add object points, image points (after refining them)\n",
        "            if ret == True:\n",
        "                print(f\"{\"Processed \"}{fname}\")\n",
        "                objpoints.append(objp)\n",
        "\n",
        "                corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)\n",
        "                imgpoints.append(corners2)\n",
        "        return(cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None))\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)\n",
        "\n",
        "def calibrate_and_display(folder_path):\n",
        "    ret, mtx, dist, rvecs, tvecs = Calibration.calibrateCamera(folder_path)\n",
        "\n",
        "    files = glob.glob(os.path.join(folder_path, \"*.jpeg\")) + glob.glob(os.path.join(folder_path, \"*.JPEG\"))\n",
        "\n",
        "    results = {\n",
        "        \"return_value\": ret,\n",
        "        \"camera_matrix\": mtx,\n",
        "        \"distortion_coefficients\": dist,\n",
        "        \"rotation_vectors\": rvecs,\n",
        "        \"translation_vectors\": tvecs,\n",
        "        \"image_paths\": files\n",
        "    }\n",
        "    json_output = json.dumps(results, cls=NumpyEncoder, indent=4)\n",
        "\n",
        "    with open(\"calibration.json\", \"w\") as f:\n",
        "        f.write(json_output)\n",
        "\n",
        "    return json_output\n",
        "\n",
        "if in_colab():\n",
        "    calibrate_and_display(\"camera-pose-estimator/images\")\n",
        "else:\n",
        "    calibrate_and_display(\"images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TcNTc1ggqBip"
      },
      "outputs": [],
      "source": [
        "class calib:\n",
        "  @staticmethod\n",
        "  def load_calibration(jsonPath:str='calibration.json'):\n",
        "    with open(jsonPath, 'r') as file:\n",
        "            calibrationDict = json.load(file)\n",
        "    #extracting k and distance coefficientse\n",
        "    k = calibrationDict['camera_matrix']\n",
        "    distanceCoeff = calibrationDict['distortion_coefficients']\n",
        "    return k, distanceCoeff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h5Ox-iFMqIOd"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "  @staticmethod\n",
        "  def load_points(csv_path:str):\n",
        "    points = np.loadtxt(csv_path)\n",
        "    return points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizer Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Visualizer:\n",
        "  def visualize(rotation_vectors, translation_vectors):\n",
        "    sensor_size = (4608, 3456) # maybe change this in case the images are different sizes?\n",
        "    virtual_image_distance = .8\n",
        "    with open(\"calibration.json\", \"r\") as file:\n",
        "        views = json.load(file)\n",
        "\n",
        "    Lambda = np.array(views.get(\"camera_matrix\")).reshape(3,3)\n",
        "    Omega = rotation_vectors\n",
        "    tau = translation_vectors\n",
        "    dist = np.array(views.get(\"distortion_coefficients\"))\n",
        "\n",
        "    for Omega_single, tau_single in zip(Omega, tau):\n",
        "\n",
        "        # Convert to a rotation matrix from a vector to work with the existing code\n",
        "        Omega, _ = cv.Rodrigues(np.array(Omega_single))\n",
        "        tau = np.array(tau_single)\n",
        "\n",
        "        # This is the camera coordinate frame\n",
        "        # Camera pose, i.e., the matrix [R t] of extrinsic parameters\n",
        "        Rt = np.block([Omega.T, -Omega.T @ tau])\n",
        "\n",
        "        # Convert Rt from 3x4 to a 4x4 transformation matrix\n",
        "        Rt = np.vstack([Rt, [0, 0, 0, 1]])\n",
        "\n",
        "        cam2world = Rt\n",
        "        ax = pt.plot_transform(A2B=cam2world, s=2,\n",
        "                            # name=\"Camera\"\n",
        "                            )\n",
        "        pc.plot_camera(\n",
        "            ax,\n",
        "            cam2world=cam2world,\n",
        "            M=Lambda,\n",
        "            sensor_size=sensor_size,\n",
        "            virtual_image_distance=virtual_image_distance,\n",
        "        )\n",
        "\n",
        "    # out of the loop\n",
        "    cam2world = pt.transform_from_pq([0, 0, 0, 0, 0, 0, 0])\n",
        "    #print(f\"Camera to world transformation:\\n{cam2world}\\n\")\n",
        "    pt.plot_transform(ax, A2B=cam2world, s=3,\n",
        "                    #name=\"World\"\n",
        "                    )\n",
        "\n",
        "\n",
        "    ax.view_init(30, 70)\n",
        "    ax.set_xlim(-15, 15)\n",
        "    ax.set_ylim(-15, 15)\n",
        "    ax.set_zlim(-15, 15)\n",
        "    ax.invert_zaxis() # It looks upside down without this\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homography --> Pose (Explicit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenCV Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Visualizer:\n",
        "  def visualize(rotation_vector, translation_vector):\n",
        "    sensor_size = (4608, 3456) # maybe change this in case the images are different sizes?\n",
        "    virtual_image_distance = .8\n",
        "    with open(\"calibration.json\", \"r\") as file:\n",
        "        views = json.load(file)\n",
        "\n",
        "    Lambda = np.array(views.get(\"camera_matrix\")).reshape(3,3)\n",
        "    Omega = np.array(rotation_vector)\n",
        "    tau = np.array(translation_vector)\n",
        "    dist = np.array(views.get(\"distortion_coefficients\"))\n",
        "\n",
        "    # Convert to a rotation matrix from a vector\n",
        "    Omega, _ = cv.Rodrigues(Omega)\n",
        "\n",
        "    # Camera pose, i.e., the matrix [R t] of extrinsic parameters\n",
        "    Rt = np.block([Omega.T, -Omega.T @ tau.reshape(3, 1)])\n",
        "\n",
        "    # Convert Rt from 3x4 to a 4x4 transformation matrix\n",
        "    Rt = np.vstack([Rt, [0, 0, 0, 1]])\n",
        "\n",
        "    cam2world = Rt\n",
        "    ax = pt.plot_transform(A2B=cam2world, s=2)\n",
        "    pc.plot_camera(\n",
        "        ax,\n",
        "        cam2world=cam2world,\n",
        "        M=Lambda,\n",
        "        sensor_size=sensor_size,\n",
        "        virtual_image_distance=virtual_image_distance,\n",
        "    )\n",
        "\n",
        "    # World frame\n",
        "    cam2world = pt.transform_from_pq([0, 0, 0, 0, 0, 0, 0])\n",
        "    pt.plot_transform(ax, A2B=cam2world, s=3)\n",
        "\n",
        "    ax.view_init(30, 70)\n",
        "    ax.set_xlim(-15, 15)\n",
        "    ax.set_ylim(-15, 15)\n",
        "    ax.set_zlim(-15, 15)\n",
        "    ax.invert_zaxis() # It looks upside down without this\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradio Picker code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eXeDhLacqJ85"
      },
      "outputs": [],
      "source": [
        "\n",
        "def launch_point_picker(my_image):\n",
        "    points_store = []\n",
        "    app = None\n",
        "    SELECTED_POINTS = None\n",
        "\n",
        "    def _to_pil_from_numpy(arr: np.ndarray) -> Image.Image:\n",
        "        arr = np.asarray(arr)\n",
        "        # channel-first -> channel-last\n",
        "        if arr.ndim == 3 and arr.shape[0] in (1,3,4) and arr.shape[-1] not in (1,3,4):\n",
        "            arr = np.transpose(arr, (1,2,0))\n",
        "        if np.issubdtype(arr.dtype, np.floating):\n",
        "            # scale floats in [0,1] to [0,255]\n",
        "            arr = (np.clip(arr, 0.0, 1.0) * 255.0).round().astype(np.uint8)\n",
        "        elif arr.dtype != np.uint8:\n",
        "            arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
        "        # Choose mode\n",
        "        if arr.ndim == 2:\n",
        "            return Image.fromarray(arr, mode=\"L\")\n",
        "        if arr.ndim == 3 and arr.shape[2] == 3:\n",
        "            return Image.fromarray(arr, mode=\"RGB\")\n",
        "        if arr.ndim == 3 and arr.shape[2] == 4:\n",
        "            return Image.fromarray(arr, mode=\"RGBA\")\n",
        "        if arr.ndim == 3 and arr.shape[2] == 1:\n",
        "            return Image.fromarray(arr[:,:,0], mode=\"L\")\n",
        "        raise ValueError(f\"Unsupported array shape: {arr.shape}\")\n",
        "\n",
        "    def _to_pil(img):\n",
        "        if isinstance(img, Image.Image):\n",
        "            return img\n",
        "        if isinstance(img, np.ndarray):\n",
        "            return _to_pil_from_numpy(img)\n",
        "        raise gr.Error(\"Set `my_image` to a PIL image or NumPy array before launching.\")\n",
        "\n",
        "    def _draw_points(base_img: Image.Image, pts, radius=5):\n",
        "        img = base_img.copy().convert(\"RGB\")\n",
        "        d = ImageDraw.Draw(img)\n",
        "        for (x, y) in pts:\n",
        "            d.ellipse([x-radius, y-radius, x+radius, y+radius], outline=(255,0,0), width=2)\n",
        "        return img\n",
        "\n",
        "    # Prepare base image from notebook variable\n",
        "    # if 'my_image' not in globals():\n",
        "        # raise RuntimeError(\"Please define `my_image` (PIL image or NumPy array) before running this cell.\")\n",
        "    # base_pil = _to_pil(globals()['my_image'])\n",
        "    base_pil = _to_pil(my_image)\n",
        "\n",
        "    def _refresh_numpy():\n",
        "        \"\"\"Return current preview (base + points) as numpy for Gradio.\"\"\"\n",
        "        return np.array(_draw_points(base_pil, points_store))\n",
        "\n",
        "    def on_click(evt: gr.SelectData):\n",
        "        # Get coordinates robustly\n",
        "        x = y = None\n",
        "        if hasattr(evt, \"index\") and evt.index is not None:\n",
        "            try: x, y = evt.index\n",
        "            except: pass\n",
        "        if (x is None or y is None) and hasattr(evt, \"x\") and hasattr(evt, \"y\"):\n",
        "            x, y = evt.x, evt.y\n",
        "        if x is None or y is None:\n",
        "            return gr.update(), json.dumps(points_store)\n",
        "\n",
        "        # Clamp to image bounds\n",
        "        w, h = base_pil.size\n",
        "        x = int(max(0, min(w-1, x)))\n",
        "        y = int(max(0, min(h-1, y)))\n",
        "\n",
        "        points_store.append([x, y])\n",
        "        return _refresh_numpy(), json.dumps(points_store)\n",
        "\n",
        "    def undo_last():\n",
        "        if points_store:\n",
        "            points_store.pop()\n",
        "        return _refresh_numpy(), json.dumps(points_store)\n",
        "\n",
        "    def clear_points():\n",
        "        points_store.clear()\n",
        "        return np.array(base_pil), \"[]\"\n",
        "\n",
        "    def done_btn_click():\n",
        "        \"\"\"Save to notebook var `selected_points` and close the app.\"\"\"\n",
        "        global SELECTED_POINTS\n",
        "        SELECTED_POINTS = [list(p) for p in points_store]\n",
        "        try:\n",
        "            ip = get_ipython()\n",
        "            if ip is not None:\n",
        "                ip.user_ns['selected_points'] = SELECTED_POINTS\n",
        "        except Exception:\n",
        "            pass\n",
        "        # threading.Thread(target=lambda: app.close(), daemon=True).start()\n",
        "        with open('selected_points.json', 'w') as f:\n",
        "            json.dump(SELECTED_POINTS, f)\n",
        "\n",
        "        # Use OpenCV to calculate camera pose\n",
        "        rotation_vector, translation_vector = estimate_pose_opencv()\n",
        "        \n",
        "        # Plot the cameras\n",
        "        Visualizer.visualize(rotation_vector, translation_vector)\n",
        "        return f\"‚úÖ Saved {len(SELECTED_POINTS)} points to `selected_points`. Closing‚Ä¶\"\n",
        "    \n",
        "\n",
        "    with gr.Blocks(title=\"Point Picker (single image)\") as demo:\n",
        "        gr.Markdown(\"**Click on the image to add points.** Use Undo / Clear as needed, then press **Done**.\")\n",
        "        img = gr.Image(\n",
        "            value=np.array(base_pil), label=\"Image (click to add points)\",\n",
        "            type=\"numpy\", interactive=True, sources=[]  # sources=[] disables uploads\n",
        "        )\n",
        "        with gr.Row():\n",
        "            undo_btn = gr.Button(\"‚Ü©Ô∏è Undo\")\n",
        "            clear_btn = gr.Button(\"üßπ Clear\")\n",
        "            done_btn = gr.Button(\"‚úÖ Done\", variant=\"primary\")\n",
        "        pts_text = gr.Textbox(label=\"Points (JSON)\", value=\"[]\", interactive=False)\n",
        "        status = gr.Markdown(\"\")\n",
        "\n",
        "        # One image used for both input and output\n",
        "        img.select(on_click, inputs=None, outputs=[img, pts_text])\n",
        "        undo_btn.click(lambda: undo_last(), outputs=[img, pts_text])\n",
        "        clear_btn.click(lambda: clear_points(), outputs=[img, pts_text])\n",
        "        done_btn.click(done_btn_click, outputs=[status])\n",
        "\n",
        "\n",
        "        app = demo.launch(inline=True, prevent_thread_lock=True)\n",
        "        return app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Launch Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "u6O4WBM7qlU2",
        "outputId": "5e532181-6628-4128-9834-0aa0aa4dd9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7863\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "selected_points: [[1689, 737], [2801, 828], [3314, 3034], [1385, 3041]]\n",
            "camera_matrix: [[3.0363083e+03 0.0000000e+00 2.2838396e+03]\n",
            " [0.0000000e+00 3.0371665e+03 1.6451669e+03]\n",
            " [0.0000000e+00 0.0000000e+00 1.0000000e+00]]\n",
            "dist_coeffs: [[ 3.0416984e-02 -1.9095460e-01 -7.1942881e-03 -4.2199690e-04\n",
            "   5.3656960e-01]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/xf/t3n3nvld6hl8s1vwwsv004mm0000gn/T/ipykernel_33777/1932130389.py:42: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "# my_image can be a PIL.Image or a NumPy array (H,W[,C])\n",
        "from PIL import Image\n",
        "if in_colab():\n",
        "    my_image = Image.open(\"camera-pose-estimator\" + \"/\" + \"DSCN0851.JPG\")\n",
        "else:\n",
        "    my_image = Image.open(\"DSCN0851.JPG\")\n",
        "\n",
        "selected_points = []\n",
        "\n",
        "app = launch_point_picker(my_image)   # launches inline, non-blocking\n",
        "\n",
        "# ... later, after clicking points and pressing \"Done\":\n",
        "print(selected_points)  # list of [x, y] pixel coordinates\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cv-env-pip",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
